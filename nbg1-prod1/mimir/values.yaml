mimir-distributed:
  # Inject S3 credentials from external secret
  global:
    extraEnvFrom:
      - secretRef:
          name: s3-credentials
    # Add cluster label for Mimir dashboards
    podAnnotations:
      prometheus.io/label-cluster: "mimir-nbg1-prod1"

  # Configure Mimir with structured config for S3 and Kafka
  mimir:
    structuredConfig:
      # Query frontend logging - log all queries
      frontend:
        log_queries_longer_than: -1s

      # Ingest storage configuration - use external Strimzi Kafka
      ingest_storage:
        enabled: true
        kafka:
          address: mimir-kafka-kafka-bootstrap.mimir.svc.cluster.local:9092
          topic: mimir-ingest
          auto_create_topic_enabled: false

      # Common S3 storage configuration
      common:
        storage:
          backend: s3
          s3:
            bucket_name: bc4-mimir-nbg1-prod1-fsn1
            endpoint: fsn1.your-objectstorage.com
            access_key_id: ${AWS_ACCESS_KEY_ID}
            secret_access_key: ${AWS_SECRET_ACCESS_KEY}
            # HTTP configuration for slow S3 responses
            http:
              idle_conn_timeout: 2m
              response_header_timeout: 5m
              max_idle_connections: 100
              max_idle_connections_per_host: 100

      # Block storage (metrics data)
      blocks_storage:
        backend: s3
        storage_prefix: blocks
        s3:
          bucket_name: bc4-mimir-nbg1-prod1-fsn1
          endpoint: fsn1.your-objectstorage.com
          access_key_id: ${AWS_ACCESS_KEY_ID}
          secret_access_key: ${AWS_SECRET_ACCESS_KEY}
          http:
            idle_conn_timeout: 2m
            response_header_timeout: 5m
            max_idle_connections: 100
            max_idle_connections_per_host: 100

      # Alertmanager storage - use different prefix to avoid conflict with blocks
      alertmanager_storage:
        backend: s3
        storage_prefix: alertmanager
        s3:
          bucket_name: bc4-mimir-nbg1-prod1-fsn1
          endpoint: fsn1.your-objectstorage.com
          access_key_id: ${AWS_ACCESS_KEY_ID}
          secret_access_key: ${AWS_SECRET_ACCESS_KEY}
          http:
            idle_conn_timeout: 2m
            response_header_timeout: 5m
            max_idle_connections: 100
            max_idle_connections_per_host: 100

      # Ruler storage - use different prefix to avoid conflict with blocks
      ruler_storage:
        backend: s3
        storage_prefix: ruler
        s3:
          bucket_name: bc4-mimir-nbg1-prod1-fsn1
          endpoint: fsn1.your-objectstorage.com
          access_key_id: ${AWS_ACCESS_KEY_ID}
          secret_access_key: ${AWS_SECRET_ACCESS_KEY}
          http:
            idle_conn_timeout: 2m
            response_header_timeout: 5m
            max_idle_connections: 100
            max_idle_connections_per_host: 100

  # Disable the built-in Kafka subchart - we use Strimzi
  kafka:
    enabled: false

  # Disable MinIO - we use external S3
  minio:
    enabled: false

  # Runtime configuration with per-tenant overrides
  runtimeConfig:
    overrides:
      # Limits for "prod" tenant (X-Scope-OrgID: prod)
      prod:
        # Increase label limit for Istio metrics (default: 30, Istio has ~44)
        max_label_names_per_series: 60
        # Ingestion limits sized for current cluster capacity
        # See post-mortem: 2025-12-27-mimir-kafka-outage.md for analysis
        ingestion_rate: 100000
        ingestion_burst_size: 200000
        max_global_series_per_user: 1500000
        # Ruler limits for Mimir mixin rules
        ruler_max_rules_per_rule_group: 100
        ruler_max_rule_groups_per_tenant: 100

  # Alertmanager - HA with 2 replicas
  alertmanager:
    enabled: true
    replicas: 2
    persistentVolume:
      enabled: true
      size: 1Gi
    resources:
      requests:
        cpu: 50m
        memory: 128Mi
      limits:
        memory: 256Mi
    statefulSet:
      enabled: true

  # Compactor
  compactor:
    replicas: 1
    persistentVolume:
      size: 50Gi
    resources:
      requests:
        cpu: 100m
        memory: 512Mi
      limits:
        memory: 1Gi

  # Distributor - scale out to reduce per-pod GC pressure
  distributor:
    replicas: 5
    resources:
      requests:
        cpu: 200m
        memory: 512Mi
      limits:
        memory: 1Gi

  # Ingester - HA with 3 replicas for partition distribution
  ingester:
    replicas: 3
    persistentVolume:
      size: 10Gi
    resources:
      requests:
        cpu: 200m
        memory: 1Gi
      limits:
        memory: 3Gi
    zoneAwareReplication:
      enabled: false
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                  - key: app.kubernetes.io/component
                    operator: In
                    values:
                      - ingester
              topologyKey: kubernetes.io/hostname

  # Querier
  querier:
    replicas: 2
    resources:
      requests:
        cpu: 100m
        memory: 256Mi
      limits:
        memory: 512Mi

  # Query Frontend
  query_frontend:
    replicas: 2
    resources:
      requests:
        cpu: 100m
        memory: 128Mi
      limits:
        memory: 512Mi

  # Query Scheduler
  query_scheduler:
    replicas: 2
    resources:
      requests:
        cpu: 50m
        memory: 64Mi
      limits:
        memory: 128Mi

  # Store Gateway - HA with 3 replicas
  store_gateway:
    replicas: 3
    persistentVolume:
      size: 5Gi
    resources:
      requests:
        cpu: 100m
        memory: 256Mi
      limits:
        memory: 512Mi
    zoneAwareReplication:
      enabled: false
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                  - key: app.kubernetes.io/component
                    operator: In
                    values:
                      - store-gateway
              topologyKey: kubernetes.io/hostname

  # Ruler
  ruler:
    enabled: true
    replicas: 1
    resources:
      requests:
        cpu: 50m
        memory: 128Mi
      limits:
        memory: 256Mi

  # Overrides exporter
  overrides_exporter:
    replicas: 1
    resources:
      requests:
        cpu: 50m
        memory: 64Mi
      limits:
        memory: 128Mi

  # Gateway (nginx)
  gateway:
    replicas: 2
    resources:
      requests:
        cpu: 50m
        memory: 64Mi
      limits:
        memory: 128Mi

  # Caches - smaller replicas for small cluster
  chunks-cache:
    enabled: true
    replicas: 2
    allocatedMemory: 512
    resources:
      requests:
        cpu: 50m
        memory: 600Mi
      limits:
        memory: 600Mi

  index-cache:
    enabled: true
    replicas: 2
    allocatedMemory: 256
    resources:
      requests:
        cpu: 50m
        memory: 300Mi
      limits:
        memory: 300Mi

  metadata-cache:
    enabled: true
    replicas: 2
    allocatedMemory: 128
    resources:
      requests:
        cpu: 50m
        memory: 150Mi
      limits:
        memory: 150Mi

  results-cache:
    enabled: true
    replicas: 2
    allocatedMemory: 128
    resources:
      requests:
        cpu: 50m
        memory: 150Mi
      limits:
        memory: 150Mi
